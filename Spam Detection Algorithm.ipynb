{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Using Natural Language Processing\n",
    "\n",
    "In this project I explored text message data and created models to predict if a message is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I look at the beginning of the dataset and encode the target labels for spam and not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam', 1, 0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine whether the dataset is balanced by calculating the percentage of spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_spam = sum(spam_data['target'])\n",
    "ratio_spam = sum_spam / len(spam_data)\n",
    "spam_percentage = ratio_spam * 100\n",
    "spam_percentage\n",
    "# data is unbalanced, with more instances of non-spam messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to determine how important certain words are to spam messages and non-spam messages, including how often they appear in these messages. So I fit and transform the training data using a Tfidf Vectorizer to determine the 20 features with the smallest and largest tf-idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name\n",
       " aaniye          0.074475\n",
       " athletic        0.074475\n",
       " chef            0.074475\n",
       " companion       0.074475\n",
       " courageous      0.074475\n",
       " dependable      0.074475\n",
       " determined      0.074475\n",
       " exterminator    0.074475\n",
       " healer          0.074475\n",
       " listener        0.074475\n",
       " organizer       0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " psychologist    0.074475\n",
       " pudunga         0.074475\n",
       " stylist         0.074475\n",
       " sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64,\n",
       " name\n",
       " 146tf150p    1.000000\n",
       " 645          1.000000\n",
       " anything     1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " done         1.000000\n",
       " er           1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " ok           1.000000\n",
       " okie         1.000000\n",
       " thank        1.000000\n",
       " thanx        1.000000\n",
       " too          1.000000\n",
       " where        1.000000\n",
       " yup          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "smallest = feature_names[sorted_tfidf_index[:20]]\n",
    "largest = feature_names[sorted_tfidf_index[:-21:-1]]\n",
    "vect_data = X_train_vectorized.max(0).toarray()[0]\n",
    "frame_small = {'value': pd.Series(vect_data[sorted_tfidf_index[:20]]), 'name': pd.Series(smallest)} \n",
    "df_small = pd.DataFrame(frame_small).sort_values(['value', 'name'])\n",
    "frame_large = {'value': pd.Series(vect_data[sorted_tfidf_index[:-21:-1]]), 'name': pd.Series(largest)} \n",
    "df_large = pd.DataFrame(frame_large).sort_values(['value', 'name'], ascending=[False, True])\n",
    "series_small = pd.Series(data = df_small['value'].values, index = df_small['name'])\n",
    "series_large = pd.Series(data = df_large['value'].values, index = df_large['name'])\n",
    "w_values = (series_small, series_large)\n",
    "w_values[0].name = None\n",
    "w_values[1].name = None\n",
    "w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I begin some feature-engineering. I determine the average length (number of characters) for spam and not-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spam_data\n",
    "data['length'] = [len(x) for x in data['text']]\n",
    "avg_spam = data['length'].loc[data['target'] == 1].mean()\n",
    "avg_not_spam = data['length'].loc[data['target'] == 0].mean()\n",
    "w_length = (avg_not_spam, avg_spam)\n",
    "w_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The following function helps combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the average number of digits for spam and not-spam messages. I plan to use this feature in my final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spam_data\n",
    "num_digits = []\n",
    "for x in data['text']:\n",
    "    number = sum(y.isdigit() for y in x)\n",
    "    num_digits.append(number)\n",
    "data['digit'] = num_digits\n",
    "digit_spam = data['digit'].loc[data['target'] == 1].mean()\n",
    "digit_not_spam = data['digit'].loc[data['target'] == 0].mean()\n",
    "w_digits = (digit_not_spam, digit_spam)\n",
    "w_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature I want to engineer and consider is the average number of non-word characters (anything other than a letter, digit, or underscore) per message for spam and not-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data = spam_data\n",
    "num_other = []\n",
    "for x in data['text']:\n",
    "    char_values = re.findall(r'\\W', x)\n",
    "    len_char = len(char_values)\n",
    "    num_other.append(len_char)\n",
    "data['other'] = num_other\n",
    "other_spam = data['other'].loc[data['target'] == 1].mean()\n",
    "other_not_spam = data['other'].loc[data['target'] == 0].mean()\n",
    "w_char = (other_not_spam, other_spam)\n",
    "w_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "Finally, I fit and transform the training data using a Count Vectorizer. I ignore terms that have a document frequency lower than 5 and I use character n-grams from n = 2 to n = 5.\n",
    "\n",
    "In the Count Vectorizer I specified the \"analyzer = 'char_wb'\" parameter in order to create character n-grams only from text inside word boundaries. This helps make the model more robust to spelling mistakes.\n",
    "\n",
    "I used the document-term matrix and the following additional features:\n",
    "- length of messages (number of characters)\n",
    "- number of digits per message\n",
    "- number of non-word characters (anything other than a letter, digit or underscore)\n",
    "\n",
    "Then, I fit a Logistic Regression model with regularization C = 100. To assess model performance, I computed the area under the curve (AUC) score using the transformed test data. Additionally, I found the 10 smallest and 10 largest coefficients from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_model():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    message_length_train = [len(x) for x in X_train]\n",
    "    message_length_test = [len(x) for x in X_test]\n",
    "    num_digits_train = []\n",
    "    num_other_train = []\n",
    "    for x in X_train:\n",
    "        number = sum(y.isdigit() for y in x)\n",
    "        num_digits_train.append(number)\n",
    "        char_values = re.findall(r'\\W', x)\n",
    "        len_char = len(char_values)\n",
    "        num_other_train.append(len_char)\n",
    "    num_digits_test = []\n",
    "    num_other_test = []\n",
    "    for w in X_test:\n",
    "        number = sum(q.isdigit() for q in w)\n",
    "        num_digits_test.append(number)\n",
    "        char_values = re.findall(r'\\W', w)\n",
    "        len_char = len(char_values)\n",
    "        num_other_test.append(len_char)\n",
    "    vect = CountVectorizer(min_df = 5, analyzer = 'char_wb', ngram_range = (2,5)).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_train_v_feat = add_feature(X_train_vectorized, [message_length_train, num_digits_train, num_other_train])\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    X_test_v_feat = add_feature(X_test_vectorized, [message_length_test, num_digits_test, num_other_test])\n",
    "    clf = LogisticRegression(C = 100)\n",
    "    clf.fit(X_train_v_feat, y_train)\n",
    "    predictions = clf.predict(X_test_v_feat)\n",
    "    auc_score = roc_auc_score(y_test, predictions)\n",
    "    feature_names = vect.get_feature_names()\n",
    "    feature_names.append('length_of_doc')\n",
    "    feature_names.append('digit_count')\n",
    "    feature_names.append('non_word_char_count')\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_coef_index = clf.coef_[0].argsort()\n",
    "    small = feature_names[sorted_coef_index[:10]]\n",
    "    large = feature_names[sorted_coef_index[:-11:-1]]\n",
    "\n",
    "    coef_data = clf.coef_.max(0) #.toarray()[0]\n",
    "    frame_small = {'value': pd.Series(coef_data[sorted_coef_index[:10]]), 'feature_name': pd.Series(small)} \n",
    "    df_small = pd.DataFrame(frame_small).sort_values(['value', 'feature_name'])\n",
    "    frame_large = {'value': pd.Series(coef_data[sorted_coef_index[:-11:-1]]), 'feature_name': pd.Series(large)} \n",
    "    df_large = pd.DataFrame(frame_large).sort_values(['value', 'feature_name'], ascending=[False, True])\n",
    "    series_small = pd.Series(data = df_small['value'].values, index = df_small['feature_name'])\n",
    "    series_large = pd.Series(data = df_large['value'].values, index = df_large['feature_name'])\n",
    "    w_final = (auc_score, series_small, series_large)\n",
    "    return w_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9813973821367333,\n",
       " feature_name\n",
       " ..    -1.356845\n",
       " .     -1.141882\n",
       "  i    -0.854167\n",
       "  go   -0.721966\n",
       "  y    -0.718508\n",
       " ?     -0.688946\n",
       " pe    -0.651174\n",
       " go    -0.618091\n",
       " h     -0.610952\n",
       " ok    -0.601233\n",
       " dtype: float64,\n",
       " feature_name\n",
       " digit_count    1.454480\n",
       " co             0.741110\n",
       " ne             0.728373\n",
       " ww             0.710818\n",
       " ar             0.636197\n",
       " ia             0.628743\n",
       " xt             0.608064\n",
       "  ch            0.604465\n",
       " mob            0.583807\n",
       "  a             0.575787\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has an AUC score of 0.98 and can successfully classify messages as either spam or not-spam."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
